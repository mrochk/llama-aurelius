{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9502029,"sourceType":"datasetVersion","datasetId":5782911}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n! pip install -U torch transformers datasets bitsandbytes peft huggingface_hub","metadata":{"_uuid":"cdf5602a-96bf-4a20-a11c-fc6129bd2aa3","_cell_guid":"4e888422-d4cc-4a22-bd91-d68e9ee80b6d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:45:22.111337Z","iopub.execute_input":"2024-09-29T14:45:22.111857Z","iopub.status.idle":"2024-09-29T14:47:58.945857Z","shell.execute_reply.started":"2024-09-29T14:45:22.111820Z","shell.execute_reply":"2024-09-29T14:47:58.944690Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"_uuid":"6b3bf6f8-138d-452b-bf2e-5b0d78db79b3","_cell_guid":"25765e1e-574d-4f72-96fa-5cc6ad139835","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's first load the model and its tokenizer\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodelname = 'meta-llama/Llama-3.2-3B'\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    modelname,\n    torch_dtype=torch.bfloat16, # setting default precision to bfloat16\n    device_map={\"\": 0}, # map model to cuda\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True, # enables 4bit quantization\n        bnb_4bit_compute_dtype=torch.bfloat16, # sets computation data type to bfloat16 for the quantization process\n        bnb_4bit_use_double_quant=True, # enables double quantization\n        bnb_4bit_quant_type='nf4', # specifying quantization type\n    )\n)\n\ntokenizer = AutoTokenizer.from_pretrained(modelname)","metadata":{"_uuid":"6fbef23f-3584-4eb0-a7a8-e957803632b6","_cell_guid":"e3a4e320-e899-4d1b-8af6-b529733649a7","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:48:30.528541Z","iopub.execute_input":"2024-09-29T14:48:30.528947Z","iopub.status.idle":"2024-09-29T14:49:02.664579Z","shell.execute_reply.started":"2024-09-29T14:48:30.528908Z","shell.execute_reply":"2024-09-29T14:49:02.663395Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0798a4f990114841be966e20cfcce55a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cad042a730f4cfabf1d1b11744426dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c758d290ef444b9630c8ce4ceadd78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a88e5ed8ed14d2db3ecd116fdc3f013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54ff09ed21054739845f3ae5d5c4c16e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79d1b5f17c948bb8f210112f1df674c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0444a6e73d8948699e0aaee1a734e2f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25d872ee956a4cb791c401bafb64c39a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2904ef1518e4871a9566f495aa25648"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f89da8ba33749318d7ece141a06a266"}},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"_uuid":"2e058b05-89df-47ee-ba22-85d9ee41bd3a","_cell_guid":"e03e70b7-e4d2-4136-8df1-559f3441af7a","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:49:03.999985Z","iopub.execute_input":"2024-09-29T14:49:04.000487Z","iopub.status.idle":"2024-09-29T14:49:04.010161Z","shell.execute_reply.started":"2024-09-29T14:49:04.000450Z","shell.execute_reply":"2024-09-29T14:49:04.009370Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 3072)\n    (layers): ModuleList(\n      (0-27): 28 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=3072, out_features=3072, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=3072, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=128256, bias=False)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from peft import LoraConfig, TaskType, get_peft_model\n\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM, # specifying the task\n    target_modules=['q_proj', 'k_proj', 'v_proj'],\n    inference_mode=False, # set up for training\n    r=16, # lora rank\n    lora_alpha=16, \n    lora_dropout=0.05,\n)\n\nmodel = get_peft_model(model, peft_config)\n\nmodel.print_trainable_parameters()","metadata":{"_uuid":"143f2aa8-be07-4db2-9b24-226c1a3c236e","_cell_guid":"775f071a-23d2-4951-b173-60cfe3e956a5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:49:18.211895Z","iopub.execute_input":"2024-09-29T14:49:18.212294Z","iopub.status.idle":"2024-09-29T14:49:18.640489Z","shell.execute_reply.started":"2024-09-29T14:49:18.212249Z","shell.execute_reply":"2024-09-29T14:49:18.639530Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"trainable params: 6,422,528 || all params: 3,219,172,352 || trainable%: 0.1995\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('/kaggle/input/meditations-marcus-aurelius/meditations.txt') as f:\n    data = map(lambda l: l.replace('\\n', ' '), f.read().split('\\n\\n'))\n    \ndata = [line for line in data if not line.startswith('BOOK')]\ndata[:5]","metadata":{"_uuid":"d1a82762-d716-4bcb-baef-6275808e168c","_cell_guid":"c34df46b-a1b1-46da-b648-6597c267f608","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:49:28.576706Z","iopub.execute_input":"2024-09-29T14:49:28.577057Z","iopub.status.idle":"2024-09-29T14:49:28.598634Z","shell.execute_reply.started":"2024-09-29T14:49:28.577026Z","shell.execute_reply":"2024-09-29T14:49:28.597729Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Provided by The Internet Classics Archive. See bottom for copyright. Available online at     http://classics.mit.edu//Antoninus/meditations.html',\n 'The Meditations By Marcus Aurelius',\n ' Translated by George Long',\n '----------------------------------------------------------------------',\n 'From my grandfather Verus I learned good morals and the government of my temper. ']"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset = Dataset.from_dict({\"text\": data})\ndataset","metadata":{"_uuid":"79c17340-83f0-4f1a-88ce-5e1da7b19aa9","_cell_guid":"27f1757d-fb9c-4eab-9d02-80f9ff22351f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:49:36.164747Z","iopub.execute_input":"2024-09-29T14:49:36.165108Z","iopub.status.idle":"2024-09-29T14:49:37.046790Z","shell.execute_reply.started":"2024-09-29T14:49:36.165075Z","shell.execute_reply":"2024-09-29T14:49:37.045827Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 525\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\n\ndef tokenize(sample):\n    return tokenizer(\n        sample,\n        return_tensors='pt',\n        padding=True,\n        truncation=True,\n        max_length=128,\n    )\n\ntokenized_dataset = dataset.map(lambda _: tokenize(_['text']), batched=True).remove_columns('text')\ntokenized_dataset","metadata":{"_uuid":"cbe16b50-28c8-49b5-821c-9ebac3090020","_cell_guid":"a94e2b2a-215f-4736-abb2-c388112a6ed4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:49:42.960236Z","iopub.execute_input":"2024-09-29T14:49:42.960594Z","iopub.status.idle":"2024-09-29T14:49:43.381707Z","shell.execute_reply.started":"2024-09-29T14:49:42.960560Z","shell.execute_reply":"2024-09-29T14:49:43.380849Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/525 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87681a0f5d104fa7868f2f98a5fd4d2c"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask'],\n    num_rows: 525\n})"},"metadata":{}}]},{"cell_type":"code","source":"%%capture\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\ntraining_args = TrainingArguments(\n    output_dir='output',\n    remove_unused_columns=False,\n    per_device_train_batch_size=8,\n    gradient_accumulation_steps=4,\n    learning_rate= 2e-4,\n    num_train_epochs=5,\n    fp16=True,\n    report_to='none',\n    logging_steps=25,\n)\n\ntrainer = Trainer(\n    model,\n    training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=DataCollatorForLanguageModeling(\n        tokenizer, \n        mlm=False, \n        return_tensors='pt',\n    ),\n)","metadata":{"_uuid":"876385a8-421f-48b7-95fb-1a2d2fe39af9","_cell_guid":"c7db4a3a-77cb-4d03-be8a-f808d73955be","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T15:15:52.744144Z","iopub.execute_input":"2024-09-29T15:15:52.745009Z","iopub.status.idle":"2024-09-29T15:15:52.782546Z","shell.execute_reply.started":"2024-09-29T15:15:52.744965Z","shell.execute_reply":"2024-09-29T15:15:52.781775Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"_uuid":"6081aa5e-67d9-409f-97df-89c4be616fe2","_cell_guid":"9df2d52a-ef99-4d0b-ae5c-722a83178e8f","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-09-29T14:50:29.007122Z","iopub.execute_input":"2024-09-29T14:50:29.008298Z","iopub.status.idle":"2024-09-29T15:03:20.322650Z","shell.execute_reply.started":"2024-09-29T14:50:29.008256Z","shell.execute_reply":"2024-09-29T15:03:20.321751Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [80/80 12:40, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>3.055100</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.815400</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.721500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=80, training_loss=2.8512081623077394, metrics={'train_runtime': 770.0549, 'train_samples_per_second': 3.409, 'train_steps_per_second': 0.104, 'total_flos': 5528473310330880.0, 'train_loss': 2.8512081623077394, 'epoch': 4.848484848484849})"},"metadata":{}}]},{"cell_type":"code","source":"model.save_pretrained('Llama-Aurelius')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:03:46.096311Z","iopub.execute_input":"2024-09-29T15:03:46.097178Z","iopub.status.idle":"2024-09-29T15:03:46.437215Z","shell.execute_reply.started":"2024-09-29T15:03:46.097133Z","shell.execute_reply":"2024-09-29T15:03:46.436161Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    './Llama-Aurelius',\n    torch_dtype=torch.bfloat16, # setting default precision to bfloat16\n    device_map={\"\": 0}, # map model to cuda\n    quantization_config=BitsAndBytesConfig(\n        load_in_4bit=True, # enables 4bit quantization\n        bnb_4bit_compute_dtype=torch.bfloat16, # sets computation data type to bfloat16 for the quantization process\n        bnb_4bit_use_double_quant=True, # enables double quantization\n        bnb_4bit_quant_type='nf4', # specifying quantization type\n    )\n)\n\ndef tokenize_inf(sample):\n    return tokenizer(sample, return_tensors='pt')","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:13:48.886447Z","iopub.execute_input":"2024-09-29T15:13:48.887184Z","iopub.status.idle":"2024-09-29T15:13:53.145260Z","shell.execute_reply.started":"2024-09-29T15:13:48.887144Z","shell.execute_reply":"2024-09-29T15:13:53.144205Z"},"trusted":true},"execution_count":36,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187239b69229427ca0eb4e4677420e5e"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = 'The best reaction against the uncertitude of life is'\ntokenized_prompt = tokenize_inf(prompt)\ninput_ids = tokenized_prompt['input_ids'].to('cuda')\nmask = tokenized_prompt['attention_mask'].to('cuda')\n\ngenerated_tokens = []\n\nwith torch.no_grad():\n    print('starting generation')\n    \n    for _ in range(100):\n        if (_+1) % 10 == 0:\n            print(f'token {_+1}/100')\n        output = model(input_ids, mask)\n    \n        last_token_logits = output.logits[:, -1, :]\n    \n        # Sample from the logits to get the next token\n        next_token_id = torch.argmax(last_token_logits, dim=-1).unsqueeze(0)  # Use argmax for deterministic output\n\n        # Append the generated token to the list\n        generated_tokens.append(next_token_id.item())\n    \n        # Update the input_ids to include the newly generated token\n        input_ids = torch.cat([input_ids, next_token_id], dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:14:36.613900Z","iopub.execute_input":"2024-09-29T15:14:36.614299Z","iopub.status.idle":"2024-09-29T15:15:02.916086Z","shell.execute_reply.started":"2024-09-29T15:14:36.614262Z","shell.execute_reply":"2024-09-29T15:15:02.915080Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"starting generation\ntoken 10/100\ntoken 20/100\ntoken 30/100\ntoken 40/100\ntoken 50/100\ntoken 60/100\ntoken 70/100\ntoken 80/100\ntoken 90/100\ntoken 100/100\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt + ''.join(list(map(tokenizer.decode, generated_tokens))).split('.')[0] + '.'","metadata":{"execution":{"iopub.status.busy":"2024-09-29T15:15:02.921809Z","iopub.execute_input":"2024-09-29T15:15:02.922162Z","iopub.status.idle":"2024-09-29T15:15:02.931014Z","shell.execute_reply.started":"2024-09-29T15:15:02.922126Z","shell.execute_reply":"2024-09-29T15:15:02.930104Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"'The best reaction against the uncertitude of life is to be content with the present, and to be satisfied with the things which are in our power.'"},"metadata":{}}]}]}