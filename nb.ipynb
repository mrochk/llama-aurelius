{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4e888422-d4cc-4a22-bd91-d68e9ee80b6d",
    "_uuid": "cdf5602a-96bf-4a20-a11c-fc6129bd2aa3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:46:29.951730Z",
     "iopub.status.busy": "2024-09-28T14:46:29.951081Z",
     "iopub.status.idle": "2024-09-28T14:46:44.063309Z",
     "shell.execute_reply": "2024-09-28T14:46:44.062147Z",
     "shell.execute_reply.started": "2024-09-28T14:46:29.951684Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install -U torch transformers datasets bitsandbytes peft huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a55aaa39-31c1-4f46-979d-79a782ab80b5",
    "_uuid": "c19049b6-2ddf-42ba-88d5-625af16a42ae"
   },
   "source": [
    "**In this notebook we are to use QLoRA to fine-tune Llama 3.1 8B on Marcus Aurelius *Meditations*, and see how well the model can mimic the Roman emperor.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "25765e1e-574d-4f72-96fa-5cc6ad139835",
    "_uuid": "6b3bf6f8-138d-452b-bf2e-5b0d78db79b3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:46:44.070029Z",
     "iopub.status.busy": "2024-09-28T14:46:44.069723Z",
     "iopub.status.idle": "2024-09-28T14:46:44.077191Z",
     "shell.execute_reply": "2024-09-28T14:46:44.076428Z",
     "shell.execute_reply.started": "2024-09-28T14:46:44.069993Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import huggingface_hub\n",
    "\n",
    "#huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "e3a4e320-e899-4d1b-8af6-b529733649a7",
    "_uuid": "6fbef23f-3584-4eb0-a7a8-e957803632b6",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:46:44.078765Z",
     "iopub.status.busy": "2024-09-28T14:46:44.078372Z",
     "iopub.status.idle": "2024-09-28T14:47:53.503497Z",
     "shell.execute_reply": "2024-09-28T14:47:53.502639Z",
     "shell.execute_reply.started": "2024-09-28T14:46:44.078721Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9b70f55d594856a483dff65cce0dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's first load the model and it's tokenizer\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "modelname = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    modelname,\n",
    "    torch_dtype=torch.bfloat16, # setting default precision to bfloat16\n",
    "    device_map={\"\": 0}, # map model to cuda\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True, # enables 4bit quantization\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, # sets computation data type to bfloat16 for the quantization process\n",
    "        bnb_4bit_use_double_quant=True, # enables double quantization\n",
    "        bnb_4bit_quant_type='nf4', # specifying quantization type\n",
    "    )\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "e03e70b7-e4d2-4136-8df1-559f3441af7a",
    "_uuid": "2e058b05-89df-47ee-ba22-85d9ee41bd3a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:53.507422Z",
     "iopub.status.busy": "2024-09-28T14:47:53.506564Z",
     "iopub.status.idle": "2024-09-28T14:47:53.517843Z",
     "shell.execute_reply": "2024-09-28T14:47:53.516961Z",
     "shell.execute_reply.started": "2024-09-28T14:47:53.507355Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "775f071a-23d2-4951-b173-60cfe3e956a5",
    "_uuid": "143f2aa8-be07-4db2-9b24-226c1a3c236e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:53.519551Z",
     "iopub.status.busy": "2024-09-28T14:47:53.519185Z",
     "iopub.status.idle": "2024-09-28T14:47:53.885241Z",
     "shell.execute_reply": "2024-09-28T14:47:53.884287Z",
     "shell.execute_reply.started": "2024-09-28T14:47:53.519516Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,359,296 || all params: 8,032,620,544 || trainable%: 0.0294\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # specifying the task\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj'],#, 'o_proj'],\n",
    "    inference_mode=False, # set up for training\n",
    "    r=4, # lora rank\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "c34df46b-a1b1-46da-b648-6597c267f608",
    "_uuid": "d1a82762-d716-4bcb-baef-6275808e168c",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:53.886713Z",
     "iopub.status.busy": "2024-09-28T14:47:53.886362Z",
     "iopub.status.idle": "2024-09-28T14:47:53.905424Z",
     "shell.execute_reply": "2024-09-28T14:47:53.904272Z",
     "shell.execute_reply.started": "2024-09-28T14:47:53.886679Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Provided by The Internet Classics Archive. See bottom for copyright. Available online at     http://classics.mit.edu//Antoninus/meditations.html',\n",
       " 'The Meditations By Marcus Aurelius',\n",
       " ' Translated by George Long',\n",
       " '----------------------------------------------------------------------',\n",
       " 'From my grandfather Verus I learned good morals and the government of my temper. ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('/kaggle/input/meditations-marcus-aurelius/meditations.txt') as f:\n",
    "    data = map(lambda l: l.replace('\\n', ' '), f.read().split('\\n\\n'))\n",
    "    \n",
    "data = [line for line in data if not line.startswith('BOOK')]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "27f1757d-fb9c-4eab-9d02-80f9ff22351f",
    "_uuid": "79c17340-83f0-4f1a-88ce-5e1da7b19aa9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:53.906979Z",
     "iopub.status.busy": "2024-09-28T14:47:53.906615Z",
     "iopub.status.idle": "2024-09-28T14:47:55.569095Z",
     "shell.execute_reply": "2024-09-28T14:47:55.568120Z",
     "shell.execute_reply.started": "2024-09-28T14:47:53.906936Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 525\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": data})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "a94e2b2a-215f-4736-abb2-c388112a6ed4",
    "_uuid": "cbe16b50-28c8-49b5-821c-9ebac3090020",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:55.570536Z",
     "iopub.status.busy": "2024-09-28T14:47:55.570189Z",
     "iopub.status.idle": "2024-09-28T14:47:55.914710Z",
     "shell.execute_reply": "2024-09-28T14:47:55.913706Z",
     "shell.execute_reply.started": "2024-09-28T14:47:55.570500Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a918935e40a34750bf2fd2fc02eb4e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/525 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 525\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(sample):\n",
    "    return tokenizer(\n",
    "        sample,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(lambda _: tokenize(_['text']), batched=True).remove_columns('text')\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c7db4a3a-77cb-4d03-be8a-f808d73955be",
    "_uuid": "876385a8-421f-48b7-95fb-1a2d2fe39af9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:47:55.916165Z",
     "iopub.status.busy": "2024-09-28T14:47:55.915855Z",
     "iopub.status.idle": "2024-09-28T14:48:04.291010Z",
     "shell.execute_reply": "2024-09-28T14:48:04.289942Z",
     "shell.execute_reply.started": "2024-09-28T14:47:55.916130Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate= 2e-4,\n",
    "    num_train_epochs=5,\n",
    "    fp16=True,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer, \n",
    "        mlm=False, \n",
    "        return_tensors='pt',\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9df2d52a-ef99-4d0b-ae5c-722a83178e8f",
    "_uuid": "6081aa5e-67d9-409f-97df-89c4be616fe2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-28T14:48:04.293260Z",
     "iopub.status.busy": "2024-09-28T14:48:04.292443Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='325' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 20/325 03:34 < 1:00:39, 0.08 it/s, Epoch 0.29/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5782911,
     "sourceId": 9502029,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30776,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
